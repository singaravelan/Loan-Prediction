{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7547843,
          "sourceType": "datasetVersion",
          "datasetId": 4395885
        }
      ],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "AVHack_2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singaravelan/Loan-Prediction/blob/master/AVHack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'avdataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4395885%2F7547843%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240208%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240208T153028Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7ffaa50cd021959072afce4b71f12e71558a338382ae0f14f09b686066170797ca642e2875abf35c316b33c5d7507dd713de692e0e0527da8fa30096e9ac785c6869144d1947408b215a2ae7c30589d0e22f822f8e3458c67da91880f3aae7ac99d8f4812c26e8f12584a55d9a2062217163af5fea4417a6154ff797ff07a8fc54101ad477c9ba6301d0126b85e06423c8709924b99b30a639d7a3269b2ac915444dc1cf390e19bea7ac9872c9246f5d2ad655f230221a66120dbf02784b1d67ec444b30eeaead5f4dc826e1e96b00faf39416381f54a3fbd1c8217eae34131e56793ad14fb21b7603aa8f82de5d987d0b7ce379a586314899cce3f5d7c64eea'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "b2f1ciIqV4ay"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-08T15:10:41.305048Z",
          "iopub.execute_input": "2024-02-08T15:10:41.305732Z",
          "iopub.status.idle": "2024-02-08T15:10:54.044212Z",
          "shell.execute_reply.started": "2024-02-08T15:10:41.305697Z",
          "shell.execute_reply": "2024-02-08T15:10:54.043387Z"
        },
        "trusted": true,
        "id": "FxBdTSpJV4a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    verbose = 1  # Verbosity\n",
        "    seed = 42  # Random seed\n",
        "    preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n",
        "    image_size = [256, 256]  # Input image size\n",
        "    epochs = 20 # Training epochs\n",
        "    batch_size = 32  # Batch size\n",
        "    lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n",
        "    drop_remainder = True  # Drop incomplete batches\n",
        "    num_classes = 2 # Number of classes in the dataset\n",
        "    fold = 0 # Which fold to set as validation data\n",
        "    class_names = [0,1]\n",
        "    label2name = dict(enumerate(class_names))\n",
        "    name2label = {v:k for k, v in label2name.items()}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-08T15:10:54.045753Z",
          "iopub.execute_input": "2024-02-08T15:10:54.046305Z",
          "iopub.status.idle": "2024-02-08T15:10:54.05226Z",
          "shell.execute_reply.started": "2024-02-08T15:10:54.046276Z",
          "shell.execute_reply": "2024-02-08T15:10:54.051358Z"
        },
        "trusted": true,
        "id": "bLyOKS23V4a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def remove_nonexistent_files(df, image_folder_column='filename', image_root_folder='.'):\n",
        "    \"\"\"\n",
        "    Remove rows from DataFrame where the files listed in the specified column do not exist.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the filenames to be checked.\n",
        "    - image_folder_column: Name of the column containing filenames.\n",
        "    - image_root_folder: Root folder where the image files are expected to be.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with rows removed for nonexistent files.\n",
        "    \"\"\"\n",
        "    # Create a function to check if a file exists\n",
        "    def file_exists(filename):\n",
        "        full_path = os.path.join(image_root_folder, filename)\n",
        "        return os.path.exists(full_path)\n",
        "\n",
        "    # Apply the function to check file existence\n",
        "    mask = df[image_folder_column].apply(file_exists)\n",
        "\n",
        "    # Filter the DataFrame to keep only rows with existing files\n",
        "    df_filtered = df[mask]\n",
        "\n",
        "    return df_filtered\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-08T15:11:59.50746Z",
          "iopub.execute_input": "2024-02-08T15:11:59.508101Z",
          "iopub.status.idle": "2024-02-08T15:11:59.514773Z",
          "shell.execute_reply.started": "2024-02-08T15:11:59.508063Z",
          "shell.execute_reply": "2024-02-08T15:11:59.513694Z"
        },
        "trusted": true,
        "id": "98aF7eSnV4a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_train_val_datasets(\n",
        "    batch_size: int = 32,\n",
        "    image_size: Tuple[int, int] = (256, 256),\n",
        "    validation_split: float = 0.2,\n",
        ") -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
        "\n",
        "    # Read the CSV file\n",
        "    data = pd.read_csv(os.path.join(\"/kaggle/input/avdataset/train/train.csv\"))\n",
        "\n",
        "\n",
        "    # Create file paths for images\n",
        "    images_path = os.path.join(\"/kaggle/input/avdataset/train/images\")\n",
        "    data=remove_nonexistent_files(data, 'filename', images_path)\n",
        "    data[\"image_id\"] = data[\"image_id\"].map(lambda x: os.path.join(images_path, f\"{x}.jpg\"))\n",
        "\n",
        "    # Map class labels to integers\n",
        "    data[\"label\"] = data[\"label\"]\n",
        "    class_name_to_label: Dict[str, int] = {\n",
        "        label: i for i, label in enumerate(set(data[\"label\"]))\n",
        "    }\n",
        "    labels: tf.Tensor = tf.constant(\n",
        "        data[\"label\"].map(class_name_to_label.__getitem__), dtype=tf.uint8\n",
        "    )\n",
        "\n",
        "    # Convert TensorFlow tensors to NumPy arrays for train_test_split\n",
        "    image_ids_np = data[\"image_id\"].to_numpy()\n",
        "    labels_np = labels.numpy()\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "        image_ids_np, labels_np, test_size=validation_split, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create TensorFlow datasets for training and validation\n",
        "    train_dataset = create_dataset(train_data, train_labels, image_size, batch_size, augment=True)\n",
        "    val_dataset = create_dataset(val_data, val_labels, image_size, batch_size)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def create_dataset(filenames, labels, image_size, batch_size, augment=False):\n",
        "    # Create a TensorFlow dataset from filenames and labels\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "\n",
        "    # Define a parsing function to read and preprocess images\n",
        "    def _parse_function(filename, label):\n",
        "        jpg_image = tf.io.decode_jpeg(tf.io.read_file(filename))\n",
        "\n",
        "        # Image augmentation (if specified)\n",
        "        if augment:\n",
        "            jpg_image = tf.image.random_flip_left_right(jpg_image)\n",
        "            jpg_image = tf.image.random_flip_up_down(jpg_image)\n",
        "            # Add more augmentations as needed\n",
        "\n",
        "        return tf.image.resize(jpg_image, size=image_size), label\n",
        "\n",
        "    # Apply the parsing function to each element in the dataset\n",
        "    dataset = dataset.map(_parse_function)\n",
        "\n",
        "    # Batch the dataset\n",
        "    return dataset.batch(batch_size)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "validation_split = 0.1\n",
        "\n",
        "train_dataset, val_dataset = get_train_val_datasets(\n",
        "    batch_size=CFG.batch_size, image_size=CFG.image_size, validation_split=validation_split\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-08T15:22:07.834172Z",
          "iopub.execute_input": "2024-02-08T15:22:07.834793Z",
          "iopub.status.idle": "2024-02-08T15:22:11.387104Z",
          "shell.execute_reply.started": "2024-02-08T15:22:07.834761Z",
          "shell.execute_reply": "2024-02-08T15:22:11.386346Z"
        },
        "trusted": true,
        "id": "3_r5QOQsV4a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = {}\n",
        "iterator = iter(val_dataset)\n",
        "# Assuming each batch contains images and labels\n",
        "images, labels = next(iterator)\n",
        "\n",
        "# Assuming labels are numerical and stored as numpy arrays\n",
        "for label in labels:\n",
        "    label_str = str(label)\n",
        "\n",
        "    if label_str in label_counts:\n",
        "        label_counts[label_str] += 1\n",
        "    else:\n",
        "        label_counts[label_str] = 1\n",
        "\n",
        "total_samples = len(labels)\n",
        "label_percentages = {label: count / total_samples * 100 for label, count in label_counts.items()}\n",
        "\n",
        "print(\"Label Percentages:\")\n",
        "for label, percentage in label_percentages.items():\n",
        "    print(f\"Label {label}: {percentage:.2f}%\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-08T15:21:39.174513Z",
          "iopub.execute_input": "2024-02-08T15:21:39.174885Z",
          "iopub.status.idle": "2024-02-08T15:21:39.31354Z",
          "shell.execute_reply.started": "2024-02-08T15:21:39.174856Z",
          "shell.execute_reply": "2024-02-08T15:21:39.312571Z"
        },
        "trusted": true,
        "id": "o4_gFWoPV4a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_images(dataset, num_images=9):\n",
        "    # Create an iterator to extract batches from the dataset\n",
        "    iterator = iter(dataset)\n",
        "\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = next(iterator)\n",
        "\n",
        "    # Plot images in a 3x3 grid\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(f\"Label: {labels[i].numpy()}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot images from the training dataset\n",
        "plot_images(train_dataset)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:05.490659Z",
          "iopub.execute_input": "2024-02-04T05:12:05.490948Z",
          "iopub.status.idle": "2024-02-04T05:12:06.375532Z",
          "shell.execute_reply.started": "2024-02-04T05:12:05.490923Z",
          "shell.execute_reply": "2024-02-04T05:12:06.374585Z"
        },
        "trusted": true,
        "id": "_reResXgV4a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "def create_test_dataset(test_data, image_folder, image_size, batch_size):\n",
        "    # Convert 'image_id' column to strings\n",
        "    test_data[\"image_id\"] = test_data[\"image_id\"].astype(str)\n",
        "\n",
        "    filenames: tf.Tensor = tf.constant(test_data[\"image_id\"], dtype=tf.string)\n",
        "    file_paths = tf.strings.join([image_folder, \"/\", filenames, \".jpg\"])\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
        "\n",
        "    def _parse_function(filename):\n",
        "        jpg_image: tf.Tensor = tf.io.decode_jpeg(tf.io.read_file(filename))\n",
        "        return tf.image.resize(jpg_image, size=image_size)\n",
        "\n",
        "    dataset = dataset.map(_parse_function)\n",
        "    return dataset.batch(batch_size)\n",
        "\n",
        "# Example usage:\n",
        "test_data = pd.read_csv(\"/kaggle/input/avdataset/test/test.csv\")  # Assuming your test CSV has a column 'image_id'\n",
        "image_folder = \"/kaggle/input/avdataset/test/images\"  # Adjust this based on your actual test image folder\n",
        "\n",
        "# Assuming 'test_image_size' and 'test_batch_size' are defined\n",
        "test_image_size = (256, 256)\n",
        "test_batch_size = 32\n",
        "\n",
        "test_dataset = create_test_dataset(test_data, image_folder, test_image_size, test_batch_size)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:06.377522Z",
          "iopub.execute_input": "2024-02-04T05:12:06.377804Z",
          "iopub.status.idle": "2024-02-04T05:12:06.407973Z",
          "shell.execute_reply.started": "2024-02-04T05:12:06.37778Z",
          "shell.execute_reply": "2024-02-04T05:12:06.407073Z"
        },
        "trusted": true,
        "id": "6IStUeifV4a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images_1(dataset, num_images=9):\n",
        "    # Create an iterator to extract batches from the dataset\n",
        "    iterator = iter(dataset)\n",
        "\n",
        "    # Get a batch of images and labels\n",
        "    images = next(iterator)\n",
        "\n",
        "    # Plot images in a 3x3 grid\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(f\"Label: \")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_images_1(test_dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:06.409267Z",
          "iopub.execute_input": "2024-02-04T05:12:06.409934Z",
          "iopub.status.idle": "2024-02-04T05:12:07.15038Z",
          "shell.execute_reply.started": "2024-02-04T05:12:06.4099Z",
          "shell.execute_reply": "2024-02-04T05:12:07.149497Z"
        },
        "trusted": true,
        "id": "WJaRuRH3V4a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "\n",
        "# Load EfficientNetB2 pre-trained model\n",
        "base_model = EfficientNetB2(weights='imagenet', include_top=False, input_shape=(CFG.image_size[0], CFG.image_size[1], 3))\n",
        "\n",
        "# Set all layers as non-trainable\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the EfficientNetB2 base model to the Sequential model\n",
        "model.add(base_model)\n",
        "\n",
        "# Add GlobalAveragePooling2D and Dropout layers\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dropout(0.2))  # Add a Dropout layer with a dropout rate of 0.2\n",
        "\n",
        "# Add a Dense layer for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:07.151902Z",
          "iopub.execute_input": "2024-02-04T05:12:07.152507Z",
          "iopub.status.idle": "2024-02-04T05:12:11.307137Z",
          "shell.execute_reply.started": "2024-02-04T05:12:07.152473Z",
          "shell.execute_reply": "2024-02-04T05:12:11.306127Z"
        },
        "trusted": true,
        "id": "AJArHKIIV4a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "# Set the training parameters\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:11.308411Z",
          "iopub.execute_input": "2024-02-04T05:12:11.308723Z",
          "iopub.status.idle": "2024-02-04T05:12:11.326302Z",
          "shell.execute_reply.started": "2024-02-04T05:12:11.308698Z",
          "shell.execute_reply": "2024-02-04T05:12:11.325373Z"
        },
        "trusted": true,
        "id": "f8zjG_6JV4a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_lr_callback(batch_size=8, mode='cos', epochs=CFG.epochs, plot=False):\n",
        "    lr_start, lr_max, lr_min = 5e-5, 6e-6 * batch_size, 1e-5\n",
        "    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n",
        "\n",
        "    def lrfn(epoch):  # Learning rate update function\n",
        "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
        "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
        "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
        "        elif mode == 'cos':\n",
        "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
        "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
        "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
        "        return lr\n",
        "\n",
        "    if plot:  # Plot lr curve if plot is True\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
        "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
        "        plt.title('LR Scheduler')\n",
        "        plt.show()\n",
        "\n",
        "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:11.327945Z",
          "iopub.execute_input": "2024-02-04T05:12:11.328272Z",
          "iopub.status.idle": "2024-02-04T05:12:11.338141Z",
          "shell.execute_reply.started": "2024-02-04T05:12:11.328242Z",
          "shell.execute_reply": "2024-02-04T05:12:11.337311Z"
        },
        "trusted": true,
        "id": "feiE5oonV4a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_cb = get_lr_callback(CFG.batch_size, mode=CFG.lr_mode, plot=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:11.34082Z",
          "iopub.execute_input": "2024-02-04T05:12:11.341111Z",
          "iopub.status.idle": "2024-02-04T05:12:11.575643Z",
          "shell.execute_reply.started": "2024-02-04T05:12:11.341087Z",
          "shell.execute_reply": "2024-02-04T05:12:11.574683Z"
        },
        "trusted": true,
        "id": "HpkaMVfpV4a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_cb = keras.callbacks.ModelCheckpoint(\"best_model.keras\",\n",
        "                                         monitor='val_loss',\n",
        "                                         save_best_only=True,\n",
        "                                         save_weights_only=False,\n",
        "                                         mode='min')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:11.576807Z",
          "iopub.execute_input": "2024-02-04T05:12:11.577096Z",
          "iopub.status.idle": "2024-02-04T05:12:11.581717Z",
          "shell.execute_reply.started": "2024-02-04T05:12:11.577072Z",
          "shell.execute_reply": "2024-02-04T05:12:11.580855Z"
        },
        "trusted": true,
        "id": "N5VLztYMV4a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(\"/kaggle/input/avdataset/train/train.csv\"))\n",
        "neg, pos = np.bincount(df['label'])\n",
        "total = neg + pos\n",
        "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:11.582958Z",
          "iopub.execute_input": "2024-02-04T05:12:11.583244Z",
          "iopub.status.idle": "2024-02-04T05:12:11.599665Z",
          "shell.execute_reply.started": "2024-02-04T05:12:11.58321Z",
          "shell.execute_reply": "2024-02-04T05:12:11.598826Z"
        },
        "trusted": true,
        "id": "EnfySEg9V4a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:11.600512Z",
          "iopub.execute_input": "2024-02-04T05:12:11.600741Z",
          "iopub.status.idle": "2024-02-04T05:12:11.606275Z",
          "shell.execute_reply.started": "2024-02-04T05:12:11.600721Z",
          "shell.execute_reply": "2024-02-04T05:12:11.605481Z"
        },
        "trusted": true,
        "id": "tNxtm2g5V4a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Note that this may take some time.\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=CFG.epochs,\n",
        "                    verbose=1,\n",
        "                    #steps_per_epoch  = math.ceil(len(train_dataset) / CFG.batch_size),\n",
        "                    validation_data=val_dataset,\n",
        "                    #callbacks=[lr_cb, ckpt_cb],\n",
        "                    class_weight=class_weight)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:12:11.607251Z",
          "iopub.execute_input": "2024-02-04T05:12:11.607529Z",
          "iopub.status.idle": "2024-02-04T05:16:13.613173Z",
          "shell.execute_reply.started": "2024-02-04T05:12:11.607494Z",
          "shell.execute_reply": "2024-02-04T05:16:13.612387Z"
        },
        "trusted": true,
        "id": "BWdBxLSbV4a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:16:13.61566Z",
          "iopub.execute_input": "2024-02-04T05:16:13.61602Z",
          "iopub.status.idle": "2024-02-04T05:16:13.891492Z",
          "shell.execute_reply.started": "2024-02-04T05:16:13.615986Z",
          "shell.execute_reply": "2024-02-04T05:16:13.890582Z"
        },
        "trusted": true,
        "id": "wZOVN1WoV4a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming you have a trained model named 'model'\n",
        "# Assuming you have a test dataset 'test_dataset' prepared using the create_test_dataset function\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "predictions_probabilities = model.predict(test_dataset)\n",
        "\n",
        "# Threshold the probabilities to get binary predictions (0 or 1)\n",
        "threshold = 0.5\n",
        "binary_predictions = (predictions_probabilities > threshold).astype(int).tolist()\n",
        "\n",
        "binary_predictions = [item for sublist in binary_predictions for item in sublist]\n",
        "\n",
        "# Create a DataFrame with image_id and predicted labels\n",
        "test_predictions_df = pd.DataFrame({\n",
        "    'image_id': test_data['image_id'],  # Assuming your test_data DataFrame has an 'image_id' column\n",
        "    'label': binary_predictions\n",
        "})\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:25:56.155395Z",
          "iopub.execute_input": "2024-02-04T05:25:56.156329Z",
          "iopub.status.idle": "2024-02-04T05:26:04.029088Z",
          "shell.execute_reply.started": "2024-02-04T05:25:56.156288Z",
          "shell.execute_reply": "2024-02-04T05:26:04.028164Z"
        },
        "trusted": true,
        "id": "WZXipxg4V4a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions_df.to_csv(\"output.csv\",index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-04T05:26:08.798313Z",
          "iopub.execute_input": "2024-02-04T05:26:08.798953Z",
          "iopub.status.idle": "2024-02-04T05:26:08.808147Z",
          "shell.execute_reply.started": "2024-02-04T05:26:08.798911Z",
          "shell.execute_reply": "2024-02-04T05:26:08.807321Z"
        },
        "trusted": true,
        "id": "UK1ngPdmV4a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wv_ckfYNV4a6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}